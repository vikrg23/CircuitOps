{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "#\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "\n",
    "# you may not use this file except in compliance with the License.\n",
    "\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#\n",
    "\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "#\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from CircuitOps Tables and Raw EDA Files and Construct TimingGCN-compatible Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "ROOT_DIR = '/raid/andlai/2024_ICCAD_Contest_Gate_Sizing_Benchmark'\n",
    "sys.path.append(ROOT_DIR)\n",
    "sys.path.append(f'{ROOT_DIR}/project')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "\n",
    "import graph_tool as gt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from project.libertyParser import libertyParser                           # https://github.com/liyanqing1987/libertyParser\n",
    "from utils.generate_LPG_from_tables import read_tables_OpenROAD_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pin_pin_df.shape, cell_pin_df.shape, net_pin_df.shape, net_cell_df.shape, cell_cell_df.shape, edge_df.shape\n",
      " (110746, 7) (85685, 6) (85685, 6) (85685, 6) (56182, 6) (423983, 2)\n"
     ]
    }
   ],
   "source": [
    "design_names = ['NV_NVDLA_partition_m', 'NV_NVDLA_partition_p', 'ariane136', 'mempool_tile_wrap']\n",
    "design_name = 'NV_NVDLA_partition_m'\n",
    "dset_name = '0709_v1'\n",
    "pin_df, cell_df, net_df, pin_pin_df, _, _, _, _, fo4_df = read_tables_OpenROAD_v2(f'{ROOT_DIR}/IR_Tables/{design_name}/')\n",
    "# pin_df, cell_df, net_df, pin_pin_df, cell_pin_df, net_pin_df, net_cell_df, cell_cell_df, fo4_df = read_tables_OpenROAD_v2(f'{ROOT_DIR}/IR_Tables/{design_name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the size files and save to pin_df\n",
    "with open(f'{ROOT_DIR}/design/{design_name}/{design_name}.size', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "sizes = []\n",
    "for line in lines:\n",
    "    sizes.append(line.replace('\\n','').split(' '))\n",
    "sizes = pd.DataFrame(sizes, columns=['cellname', 'ref'])\n",
    "sizes = pd.Series(sizes['ref'].values, index=sizes['cellname'])\n",
    "\n",
    "pin_df['ref_opt'] = pin_df['cellname'].map(sizes)\n",
    "assert pin_df['ref_opt'].isna().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drop_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(src_id) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tar_id) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m         drop_idx\u001b[38;5;241m.\u001b[39mappend(pin_pin_df\u001b[38;5;241m.\u001b[39mindex[(pin_pin_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m src_id[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m&\u001b[39m (pin_pin_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtar_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m tar_id[\u001b[38;5;241m0\u001b[39m])]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m drop_idx \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mIndex(\u001b[43mdrop_idx\u001b[49m)\n\u001b[1;32m     10\u001b[0m pin_pin_df\u001b[38;5;241m.\u001b[39mdrop(index\u001b[38;5;241m=\u001b[39mdrop_idx, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drop_idx' is not defined"
     ]
    }
   ],
   "source": [
    "# take away three specific pin-net-pin connections in ariane136 to make it a DAG\n",
    "if design_name == 'ariane136':\n",
    "    drop_idx = []\n",
    "    for src_name, tar_name in [['g1407995/Y', 'g1410303/A'], ['g1405562/Y', 'g1406560/B'], ['g1404610/Y', 'g1405522/B1']]:\n",
    "        src_id = pin_df['id'][pin_df['name'] == src_name].values\n",
    "        tar_id = pin_df['id'][pin_df['name'] == tar_name].values\n",
    "        assert len(src_id) == 1 and len(tar_id) == 1\n",
    "        drop_idx.append(pin_pin_df.index[(pin_pin_df['src_id'] == src_id[0]) & (pin_pin_df['tar_id'] == tar_id[0])].values[0])\n",
    "    drop_idx = pd.Index(drop_idx)\n",
    "    pin_pin_df.drop(index=drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove all the macros pins + cells + nets\n",
    "# macro_ids = pin_df[pin_df['is_macro'] == 1]['id'].values\n",
    "# assert False not in ((macro_ids[1:] - macro_ids[:-1]) == 1)\n",
    "# pin_pin_df.drop(index = pin_pin_df.index[pin_pin_df['src_id'].isin(macro_ids) | pin_pin_df['tar_id'].isin(macro_ids)], inplace=True)\n",
    "# pin_df.drop(index = pin_df.index[pin_df['is_macro'] == 1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard pins that are not connected to anything\n",
    "isolate_ids = set(pin_df['id']).difference(set(pin_pin_df['src_id']).union(set(pin_pin_df['tar_id'])))\n",
    "pin_df.drop(index = pin_df.index[pin_df['id'].isin(isolate_ids)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset indices\n",
    "pin_df.reset_index(inplace=True, drop=True)\n",
    "pin_pin_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# create new columns for original ids\n",
    "pin_df['org_id'] = pin_df['id']\n",
    "pin_pin_df['org_src_id'] = pin_pin_df['src_id']\n",
    "pin_pin_df['org_tar_id'] = pin_pin_df['tar_id']\n",
    "\n",
    "# create mapping from original id to new id\n",
    "map_pin_id = pd.Series(pin_df.index, pin_df['org_id'])\n",
    "pin_df['id'] = pin_df['org_id'].map(map_pin_id)\n",
    "pin_pin_df['src_id'] = pin_pin_df['src_id'].map(map_pin_id)\n",
    "pin_pin_df['tar_id'] = pin_pin_df['tar_id'].map(map_pin_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[s_port] property in pin_df is invalid. Extract from .v file\n",
      "split [netname] by '[' and take first item\n",
      "There are 2138 port pins\n"
     ]
    }
   ],
   "source": [
    "### modify is_port\n",
    "# A quick parser to extract all I/O netnames from verilog netlist\n",
    "def get_port_nets(fullpath: str):\n",
    "    start_reading_IO = False\n",
    "    with open(fullpath, 'r') as file:\n",
    "        # read until first IO line\n",
    "        while not start_reading_IO:\n",
    "            line = file.readline().replace('\\n', '')\n",
    "            if 'input' in line.split(' ') or 'output'in line.split(' '):\n",
    "                start_reading_IO = True\n",
    "\n",
    "        IOnets = [line.split(' ')[-1].replace(';','')]\n",
    "        # read until first wire line\n",
    "        while True:\n",
    "            line = file.readline().replace('\\n', '')\n",
    "            if 'input' in line.split(' ') or 'output'in line.split(' '):\n",
    "                IOnets += [line.split(' ')[-1].replace(';','')]\n",
    "            elif 'wire' in line.split(' '):\n",
    "                break\n",
    "    return IOnets\n",
    "IOnets = get_port_nets(f'{ROOT_DIR}/design/{design_name}/{design_name}.v')\n",
    "\n",
    "print('[s_port] property in pin_df is invalid. Extract from .v file')\n",
    "print('split [netname] by \\'[\\' and take first item')\n",
    "pin_df['netname_prefix'] = pin_df['netname'].str.replace('\\\\', '').str.split('[', expand=True)[0]\n",
    "\n",
    "pin_df['is_port'] = pin_df['netname_prefix'].isin(IOnets)\n",
    "print('There are', pin_df['is_port'].sum(), 'port pins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary (0.0 0.0) - (94932.0 94788.0)\n"
     ]
    }
   ],
   "source": [
    "### add to-boundary distances\n",
    "# A quick parser to extract die boundaries\n",
    "def get_die_boundaries(fullpath: str):\n",
    "    with open(fullpath, 'r') as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if 'DIEAREA' in line:\n",
    "                lx, by, rx, ty = np.array(line.split(' '))[[2,3,6,7]]\n",
    "                return float(lx), float(by), float(rx), float(ty)\n",
    "lx, by, rx, ty = get_die_boundaries(f'{ROOT_DIR}/design/{design_name}/{design_name}.def')\n",
    "print(f'Boundary ({lx} {by}) - ({rx} {ty})')\n",
    "\n",
    "pin_df['to_top'] = ty - pin_df['y']\n",
    "pin_df['to_left'] = pin_df['x'] - lx\n",
    "pin_df['to_right'] = rx - pin_df['x']\n",
    "pin_df['to_bottom'] = pin_df['y'] - by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing asap7sc7p5t_AO_RVT_FF_nldm_201020.lib\n",
      "Parsing asap7sc7p5t_INVBUF_RVT_FF_nldm_201020.lib\n",
      "Parsing asap7sc7p5t_OA_RVT_FF_nldm_201020.lib\n",
      "Parsing asap7sc7p5t_SEQ_RVT_FF_nldm_201020.lib\n",
      "Parsing asap7sc7p5t_SIMPLE_RVT_FF_nldm_201020.lib\n"
     ]
    }
   ],
   "source": [
    "# LUT extraction for edges\n",
    "lib_dir = f'{ROOT_DIR}/platform/ASAP7/lib'\n",
    "libfiles = [libfile for libfile in sorted(os.listdir(lib_dir)) if libfile[:5] != 'sram_'] # discard macros\n",
    "\n",
    "libs = []\n",
    "for libfile in libfiles:\n",
    "    if libfile[-4:] == '.lib':\n",
    "        print(f'Parsing {libfile}')\n",
    "        libs.append( libertyParser.libertyParser(f'{ROOT_DIR}/platform/ASAP7/lib/{libfile}') )\n",
    "\n",
    "cell_edges = pin_pin_df[pin_pin_df['is_net'] == 0].copy()\n",
    "cell_edges['src_pin_name'] = cell_edges['src'].str.split('/').str[-1]\n",
    "cell_edges['tar_pin_name'] = cell_edges['tar'].str.split('/').str[-1]\n",
    "cell_edges['cellname'] = cell_edges['src'].str.split('/').str[:-1].str.join('/')\n",
    "libcell_lookup = pd.Series(cell_df['ref'].values, index=cell_df['name'])\n",
    "cell_edges['ref'] = cell_edges['cellname'].map(libcell_lookup)\n",
    "\n",
    "# save cell_edge additional information to pin_pin_df\n",
    "for col in cell_edges.columns:\n",
    "    if col not in pin_pin_df.columns:\n",
    "        pin_pin_df[col] = None\n",
    "        pin_pin_df.loc[pin_pin_df['is_net'] == 0, col] = cell_edges[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cells used in this design\n",
    "used_refs = set(pin_df['ref'][pin_df['is_macro'] == 0].values)\n",
    "info_cells = []\n",
    "for lib in libs:\n",
    "    for cell in lib.getCellList():\n",
    "        if cell in used_refs:\n",
    "            info_cells.append(cell)\n",
    "info_cells = set(info_cells)\n",
    "no_info_refs = used_refs.difference(info_cells)\n",
    "assert len(no_info_refs) == 0, f'No info: {no_info_refs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER\n",
      "16 different BUFF_LUTs, indexed by (cell, src, tar), for [NV_NVDLA_partition_m], are constructed\n"
     ]
    }
   ],
   "source": [
    "# extract the LUTs for BUFFER!!!\n",
    "print('BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER')\n",
    "\n",
    "arc_types = ['cell_rise', 'cell_fall', 'rise_transition', 'fall_transition']\n",
    "parseLUT = lambda tab : np.array([row.split(',') for row in tab.replace(' ', '').replace('(\\\"','').replace('\\\")','').split('\\\",\\\"')]).astype(float)\n",
    "parseLUTidx = lambda idxs : np.array(idxs.replace(' ','').replace('(\\\"','').replace('\\\")','').split(',')).astype(float)\n",
    "BUFF_LUTs = {}\n",
    "for lib in libs:\n",
    "    # create a mapping from group name to index in lib dict for faster query\n",
    "    group_names = [group['name' ]for group in lib.libDic['group']]\n",
    "    group_map = pd.Series(np.arange(len(group_names)), index=group_names)\n",
    "    for cell in lib.getCellList():\n",
    "        found = False\n",
    "        for pref in ['BUFx', 'HB1x', 'HB2x', 'HB3x', 'HB4x']:\n",
    "            if pref == cell[:4]:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            continue\n",
    "\n",
    "        ### Section for extracting LUTs\n",
    "        # extract the src and tar pin names, and get pin info from liberty\n",
    "        src_tar = cell_edges[['src_pin_name', 'tar_pin_name']][cell_edges['ref'] == cell]\n",
    "        pin_pairs = set([*zip(src_tar['src_pin_name'], src_tar['tar_pin_name'])])\n",
    "        pins = lib.getLibPinInfo(cellList=[cell])['cell'][cell]['pin']\n",
    "\n",
    "        # for each src/tar pin pair, extract the LUT for timing\n",
    "        pin_pairs = {('A', 'Y')}\n",
    "        for src_pin, tar_pin in pin_pairs:\n",
    "            pin_stat = pins[tar_pin]['timing']\n",
    "            \n",
    "            LUTmat, LUTidx = [], []\n",
    "            for one_arc in pin_stat:\n",
    "                # there might be multiple timing arcs for one src/tar pair\n",
    "                # try to extract the arc where the 'when' key is not specified\n",
    "                if one_arc['related_pin'] != f'\\\"{src_pin}\\\"':\n",
    "                    continue\n",
    "                for arc_type in arc_types:\n",
    "                    LUTidx.append(parseLUTidx(one_arc['table_type'][arc_type]['index_1']))\n",
    "                    LUTidx.append(parseLUTidx(one_arc['table_type'][arc_type]['index_2']))\n",
    "                    LUTmat.append(parseLUT(one_arc['table_type'][arc_type]['values']))\n",
    "            LUTidx = np.array(LUTidx).reshape(-1, 4, 2, 7)\n",
    "            LUTmat = np.array(LUTmat).reshape(-1, 4, 7, 7)\n",
    "\n",
    "            # assert lengths are the same\n",
    "            assert LUTmat.shape[0] == LUTidx.shape[0]\n",
    "            # all the indices must be the same!\n",
    "            assert True not in (LUTidx.max(axis=0) != LUTidx.min(axis=0))\n",
    "            LUTidx = LUTidx[0]\n",
    "\n",
    "            # take the values of the matrix with the worst average\n",
    "            worst_ids = LUTmat.mean(axis=(2, 3)).argmax(axis=0)\n",
    "            LUTmat = np.array([LUTmat[idx1, idx2] for idx1, idx2 in zip(worst_ids, np.arange(len(arc_types)))])\n",
    "\n",
    "            # save to dict that is indexed with (cell, src_pin, tar_pin)\n",
    "            BUFF_LUTs[(cell, src_pin, tar_pin)] = {'LUTidx': LUTidx, 'LUTmat': LUTmat}\n",
    "\n",
    "print(f'{len(BUFF_LUTs.keys())} different BUFF_LUTs, indexed by (cell, src, tar), for [{design_name}], are constructed')\n",
    "\n",
    "with open(f'{ROOT_DIR}/datasets/{dset_name}/BUFF_LUTs.pk', 'wb') as pkf:\n",
    "    pk.dump(BUFF_LUTs, pkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER\n"
     ]
    }
   ],
   "source": [
    "# extract rise/fall capacitances for BUFFER!!!\n",
    "print('BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER BUFFER')\n",
    "\n",
    "# pin_df['pinname'] = [lst[-1] for lst in pin_df['name'].str.split('/')]\n",
    "RiseCaps, FallCaps = pd.Series(), pd.Series()\n",
    "for lib in libs:\n",
    "    # create a mapping from group name to index in lib dict for faster query\n",
    "    group_names = [group['name' ]for group in lib.libDic['group']]\n",
    "    group_map = pd.Series(np.arange(len(group_names)), index=group_names)\n",
    "    for cell in lib.getCellList():\n",
    "        found = False\n",
    "        for pref in ['BUFx', 'HB1x', 'HB2x', 'HB3x', 'HB4x']:\n",
    "            if pref == cell[:4]:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            continue\n",
    "        \n",
    "        ### Section for extracting rise/fall capacitances for input pins (dir = 1)\n",
    "        # extract the src pin names, and get pin info from liberty\n",
    "        # src_pins = set(pin_df['pinname'][(pin_df['ref'] == cell) & (pin_df['dir'] == 1)])\n",
    "        src_pins = {'A'}\n",
    "        cell_group = lib.libDic['group'][group_map[cell]]['group']\n",
    "        found_src_pins = []\n",
    "        for cell_attr in cell_group:\n",
    "            if cell_attr['name'] in src_pins:\n",
    "                src_pin = cell_attr['name']\n",
    "                found_src_pins.append(src_pin)\n",
    "                RiseCaps[cell] = cell_attr['rise_capacitance']\n",
    "                FallCaps[cell] = cell_attr['fall_capacitance']\n",
    "        # assert that we can find all input pins\n",
    "        assert set(found_src_pins) == src_pins\n",
    "\n",
    "BUFF_rise_fall_caps = pd.concat([pd.DataFrame(RiseCaps, columns=['rise_cap']), pd.DataFrame(FallCaps, columns=['fall_cap'])], axis=1)\n",
    "BUFF_rise_fall_caps.to_csv(f'{ROOT_DIR}/datasets/{dset_name}/BUFF_rise_fall_caps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 different LUTs, indexed by (cell, src, tar), for [NV_NVDLA_partition_m], are constructed\n"
     ]
    }
   ],
   "source": [
    "# extract the LUTs\n",
    "arc_types = ['cell_rise', 'cell_fall', 'rise_transition', 'fall_transition']\n",
    "parseLUT = lambda tab : np.array([row.split(',') for row in tab.replace(' ', '').replace('(\\\"','').replace('\\\")','').split('\\\",\\\"')]).astype(float)\n",
    "parseLUTidx = lambda idxs : np.array(idxs.replace(' ','').replace('(\\\"','').replace('\\\")','').split(',')).astype(float)\n",
    "LUTs = {}\n",
    "for lib in libs:\n",
    "    # create a mapping from group name to index in lib dict for faster query\n",
    "    group_names = [group['name' ]for group in lib.libDic['group']]\n",
    "    group_map = pd.Series(np.arange(len(group_names)), index=group_names)\n",
    "    for cell in lib.getCellList():\n",
    "        if cell not in info_cells:\n",
    "            continue\n",
    "\n",
    "        ### Section for extracting LUTs\n",
    "        # extract the src and tar pin names, and get pin info from liberty\n",
    "        src_tar = cell_edges[['src_pin_name', 'tar_pin_name']][cell_edges['ref'] == cell]\n",
    "        pin_pairs = set([*zip(src_tar['src_pin_name'], src_tar['tar_pin_name'])])\n",
    "        pins = lib.getLibPinInfo(cellList=[cell])['cell'][cell]['pin']\n",
    "\n",
    "        # for each src/tar pin pair, extract the LUT for timing\n",
    "        for src_pin, tar_pin in pin_pairs:\n",
    "            pin_stat = pins[tar_pin]['timing']\n",
    "            \n",
    "            LUTmat, LUTidx = [], []\n",
    "            for one_arc in pin_stat:\n",
    "                # there might be multiple timing arcs for one src/tar pair\n",
    "                # try to extract the arc where the 'when' key is not specified\n",
    "                if one_arc['related_pin'] != f'\\\"{src_pin}\\\"':\n",
    "                    continue\n",
    "                for arc_type in arc_types:\n",
    "                    LUTidx.append(parseLUTidx(one_arc['table_type'][arc_type]['index_1']))\n",
    "                    LUTidx.append(parseLUTidx(one_arc['table_type'][arc_type]['index_2']))\n",
    "                    LUTmat.append(parseLUT(one_arc['table_type'][arc_type]['values']))\n",
    "            LUTidx = np.array(LUTidx).reshape(-1, 4, 2, 7)\n",
    "            LUTmat = np.array(LUTmat).reshape(-1, 4, 7, 7)\n",
    "\n",
    "            # assert lengths are the same\n",
    "            assert LUTmat.shape[0] == LUTidx.shape[0]\n",
    "            # all the indices must be the same!\n",
    "            assert True not in (LUTidx.max(axis=0) != LUTidx.min(axis=0))\n",
    "            LUTidx = LUTidx[0]\n",
    "\n",
    "            # take the values of the matrix with the worst average\n",
    "            worst_ids = LUTmat.mean(axis=(2, 3)).argmax(axis=0)\n",
    "            LUTmat = np.array([LUTmat[idx1, idx2] for idx1, idx2 in zip(worst_ids, np.arange(len(arc_types)))])\n",
    "\n",
    "            # save to dict that is indexed with (cell, src_pin, tar_pin)\n",
    "            LUTs[(cell, src_pin, tar_pin)] = {'LUTidx': LUTidx, 'LUTmat': LUTmat}\n",
    "\n",
    "print(f'{len(LUTs.keys())} different LUTs, indexed by (cell, src, tar), for [{design_name}], are constructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract rise/fall capacitances\n",
    "pin_df['pinname'] = [lst[-1] for lst in pin_df['name'].str.split('/')]\n",
    "RiseCaps, FallCaps = pd.Series(), pd.Series()\n",
    "for lib in libs:\n",
    "    # create a mapping from group name to index in lib dict for faster query\n",
    "    group_names = [group['name' ]for group in lib.libDic['group']]\n",
    "    group_map = pd.Series(np.arange(len(group_names)), index=group_names)\n",
    "    for cell in lib.getCellList():\n",
    "        if cell not in info_cells:\n",
    "            continue\n",
    "        \n",
    "        ### Section for extracting rise/fall capacitances for input pins (dir = 1)\n",
    "        # extract the src pin names, and get pin info from liberty\n",
    "        src_pins = set(pin_df['pinname'][(pin_df['ref'] == cell) & (pin_df['dir'] == 1)])\n",
    "        cell_group = lib.libDic['group'][group_map[cell]]['group']\n",
    "        found_src_pins = []\n",
    "        for cell_attr in cell_group:\n",
    "            if cell_attr['name'] in src_pins:\n",
    "                src_pin = cell_attr['name']\n",
    "                found_src_pins.append(src_pin)\n",
    "                RiseCaps[f'{cell}/{src_pin}'] = cell_attr['rise_capacitance']\n",
    "                FallCaps[f'{cell}/{src_pin}'] = cell_attr['fall_capacitance']\n",
    "        # assert that we can find all input pins\n",
    "        assert set(found_src_pins) == src_pins\n",
    "\n",
    "# ### set the rise/fall capacitances by mapping\n",
    "pin_df['rise_cap'] = pin_df['ref'].str.cat(pin_df['pinname'], sep='/').map(RiseCaps)\n",
    "pin_df['fall_cap'] = pin_df['ref'].str.cat(pin_df['pinname'], sep='/').map(FallCaps)\n",
    "assert False not in (pin_df['rise_cap'].isna() == pin_df['fall_cap'].isna())\n",
    "assert pin_df['rise_cap'][(pin_df['is_macro'] == 0) & (pin_df['dir'] == 1)].isna().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to dataset 0709_v1/ariane136\n"
     ]
    }
   ],
   "source": [
    "# save pin_df, pin_pin_df, LUTs\n",
    "print(f'Saving to dataset {dset_name}/{design_name}')\n",
    "\n",
    "if not os.path.isdir(f'{ROOT_DIR}/datasets/{dset_name}'):\n",
    "    os.mkdir(f'{ROOT_DIR}/datasets/{dset_name}')\n",
    "if not os.path.isdir(f'{ROOT_DIR}/datasets/{dset_name}/{design_name}'):\n",
    "    os.mkdir(f'{ROOT_DIR}/datasets/{dset_name}/{design_name}')\n",
    "\n",
    "with open(f'{ROOT_DIR}/datasets/{dset_name}/{design_name}/LUTs.pk', 'wb') as pkf:\n",
    "    pk.dump(LUTs, pkf)\n",
    "pin_df.to_csv(f'{ROOT_DIR}/datasets/{dset_name}/{design_name}/pin_df.csv')\n",
    "pin_pin_df.to_csv(f'{ROOT_DIR}/datasets/{dset_name}/{design_name}/pin_pin_df.csv')\n",
    "net_df.to_csv(f'{ROOT_DIR}/datasets/{dset_name}/{design_name}/net_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
