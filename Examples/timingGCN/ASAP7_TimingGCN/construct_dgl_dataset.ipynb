{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "#\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "\n",
    "# you may not use this file except in compliance with the License.\n",
    "\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#\n",
    "\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "#\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/yba/miniconda3/envs/andlai_dgl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### This notebook constructs TimingGCN compatible dgl-based datasets from the ASAP7/CircuitOps design datasets\n",
    "import os, sys\n",
    "ROOT_DIR = '/raid/andlai/2024_ICCAD_Contest_Gate_Sizing_Benchmark'\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dgl\n",
    "import networkx as nx\n",
    "# import graph_tool as gt\n",
    "# from graph_tool.all import *\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2586825/188868267.py:12: DtypeWarning: Columns (10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pin_pin_df = pd.read_csv(f'{design_dir}/pin_pin_df.csv', index_col=0)\n"
     ]
    }
   ],
   "source": [
    "## load all datasets in design_names = ['NV_NVDLA_partition_m', 'NV_NVDLA_partition_p', 'ariane136', 'mempool_tile_wrap']\n",
    "design_names = ['mempool_tile_wrap']\n",
    "dataset_dir = '0709_v1'\n",
    "\n",
    "for design_name in design_names:\n",
    "    design_dir = f'{ROOT_DIR}/datasets/{dataset_dir}/{design_name}'\n",
    "    with open(f'{design_dir}/LUTs.pk', 'rb') as pkf:\n",
    "        LUTs = pk.load(pkf)\n",
    "    with open(f'{ROOT_DIR}/datasets/{dataset_dir}/BUFF_LUTs.pk', 'rb') as pkf:\n",
    "        BUFF_LUTs = pk.load(pkf)\n",
    "    pin_df = pd.read_csv(f'{design_dir}/pin_df.csv', index_col=0)\n",
    "    pin_pin_df = pd.read_csv(f'{design_dir}/pin_pin_df.csv', index_col=0)\n",
    "    net_df = pd.read_csv(f'{design_dir}/net_df.csv', index_col=0)\n",
    "    fo4_df = pd.read_csv(f'{ROOT_DIR}/datasets/{dataset_dir}/fo4_df.csv', index_col=0)\n",
    "\n",
    "    # assert there are no isolated pins\n",
    "    edge_pin_ids = np.array(sorted(list(set(pin_pin_df['src_id']).union(set(pin_pin_df['tar_id'])))))\n",
    "    assert edge_pin_ids.shape[0] == len(pin_df) and edge_pin_ids[-1] + 1 == len(pin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.34043033e-01,  2.85973675e-02,  0.00000000e+00, -1.35529000e+02,\n",
       "        -1.25820000e+02]),\n",
       " array([186.3338585 ,  53.36032457,  18.62685043, 158.802     ,\n",
       "        113.652     ]),\n",
       " array([19.41016498,  5.97311784,  1.99895321,  9.36068682,  8.96855636]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### create the features for the graph\n",
    "# pin input variables\n",
    "is_prim_IO = pin_df[['is_port']].values.astype(float)\n",
    "is_fanout = (pin_df[['dir']] == 0).astype(float)\n",
    "rel_pos = pin_df[['to_top', 'to_left', 'to_right', 'to_bottom']].values.astype(float)\n",
    "cap_info = pin_df[['rise_cap', 'fall_cap']].values.astype(float)\n",
    "node_features = np.concatenate((is_prim_IO, is_fanout, rel_pos, cap_info), axis=1)\n",
    "\n",
    "# create LUTs and corresponding key for mapping the cell variables\n",
    "LUTs_flat  = {}\n",
    "for key, LUT in LUTs.items():\n",
    "    LUT_flat = LUT['LUTidx'].reshape(4, -1)\n",
    "    # LUT_flat = np.log(LUT['LUTidx']).reshape(4, -1)                                     #### TAKE LOG\n",
    "    LUT_flat = np.concatenate((np.ones((4, 1)), LUT_flat), axis=1).flatten()\n",
    "    LUT_flat = np.concatenate((LUT_flat, LUT['LUTmat'].flatten()))\n",
    "    # LUT_flat = np.concatenate((LUT_flat, np.log(LUT['LUTmat']).flatten()))              #### TAKE LOG\n",
    "    LUTs_flat[key] = LUT_flat\n",
    "pin_cell_pin_df = pin_pin_df[pin_pin_df['is_net'] == 0]\n",
    "cell_edge_LUT_keys = [(ref, src, tar) for ref, src, tar in zip(pin_cell_pin_df['ref'], pin_cell_pin_df['src_pin_name'], pin_cell_pin_df['tar_pin_name'])]\n",
    "assert False not in [key in LUTs_flat.keys() for key in cell_edge_LUT_keys]\n",
    "\n",
    "# cell edge input variables\n",
    "LUTs_map = pd.Series([val for _, val in LUTs_flat.items()], LUTs_flat.keys())\n",
    "cell_edge_info = np.array(list(pd.Series(cell_edge_LUT_keys).map(LUTs_map)))\n",
    "\n",
    "# net edge input variables\n",
    "pin_net_pin_df = pin_pin_df[pin_pin_df['is_net'] == 1].copy()\n",
    "pin_net_pin_df['netname'] = pin_net_pin_df['src_id'].map(pd.Series(pin_df['netname'], index=pin_df['id']))\n",
    "pin_x_map = pd.Series(pin_df['x'], pin_df['id'])\n",
    "pin_y_map = pd.Series(pin_df['y'], pin_df['id'])\n",
    "src_x = pin_net_pin_df['src_id'].map(pin_x_map)\n",
    "src_y = pin_net_pin_df['src_id'].map(pin_y_map)\n",
    "tar_x = pin_net_pin_df['tar_id'].map(pin_x_map)\n",
    "tar_y = pin_net_pin_df['tar_id'].map(pin_y_map)\n",
    "# append total_cap, net_cap, net_res to net info\n",
    "net_indiv_info = []\n",
    "for net_attrs in ['total_cap', 'net_cap', 'net_res']:\n",
    "    net_map = pd.Series(net_df[net_attrs].values, index=net_df['name'])\n",
    "    net_indiv_info.append(pin_net_pin_df['netname'].map(net_map).values)\n",
    "net_indiv_info = np.stack(net_indiv_info, axis=1)\n",
    "\n",
    "#### TAKE LOG\n",
    "# net_indiv_info[net_indiv_info == 0] = 1e-3\n",
    "# net_indiv_info = np.log(net_indiv_info)\n",
    "net_out_edge_info = np.stack(((tar_x - src_x).values, (tar_y - src_y).values), axis=1)\n",
    "net_out_edge_info = np.concatenate((net_indiv_info, net_out_edge_info), axis=1)\n",
    "\n",
    "# individually make some values reasonable\n",
    "net_out_edge_info[:, :2] *= 1e15\n",
    "net_out_edge_info[:, 2] *= 1e-3\n",
    "net_out_edge_info[:, 3:] *= 1e-3\n",
    "# net_in_edge_info = -net_out_edge_info\n",
    "\n",
    "net_out_edge_info.min(axis=0), net_out_edge_info.max(axis=0), net_out_edge_info.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the tasks for the graph\n",
    "task_targets = ['tran', 'slack', 'risearr', 'fallarr', 'is_end']\n",
    "task_df = pin_df[task_targets].copy()\n",
    "task_df[(pin_df['is_macro'] == 1) | (pin_df['is_seq'] == 1)] = np.nan\n",
    "\n",
    "# create the training mask for non macro and non sequential pins\n",
    "train_mask = (pin_df['is_macro'].values == 0) & (pin_df['is_seq'].values == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8/8 add -- cell-graph to pin-graph transfer\n",
    "# generate look-up table\n",
    "cell_graph_dset_name = '0726_v1'\n",
    "cell_name_to_id = pd.read_csv(f'{ROOT_DIR}/datasets/{cell_graph_dset_name}/{design_name}/cell_df.csv', index_col=0)[['name', 'id']]\n",
    "cell_name_to_id = pd.Series(cell_name_to_id['id'].values, index=cell_name_to_id['name'])\n",
    "\n",
    "# create cell ids for 'cell_out' edges and 'pin' nodes\n",
    "# 'pin'\n",
    "cell_id_of_pins = pin_df['cellname'].map(cell_name_to_id).values\n",
    "cell_id_of_pins[np.isnan(cell_id_of_pins)] = -1e8\n",
    "cell_id_of_pins = cell_id_of_pins.astype(int)\n",
    "# 'cell_out' edges\n",
    "cell_id_of_cell_out = pin_cell_pin_df['cellname'].map(cell_name_to_id).values\n",
    "cell_id_of_cell_out[np.isnan(cell_id_of_cell_out)] = -1e8\n",
    "cell_id_of_cell_out = cell_id_of_cell_out.astype(int)\n",
    "\n",
    "# create BUFF_LUTs and corresponding key for mapping the cell variables\n",
    "BUFF_LUTs_flat  = {}\n",
    "for key, LUT in BUFF_LUTs.items():\n",
    "    LUT_flat = LUT['LUTidx'].reshape(4, -1)\n",
    "    # LUT_flat = np.log(LUT['LUTidx']).reshape(4, -1)                                     #### TAKE LOG\n",
    "    LUT_flat = np.concatenate((np.ones((4, 1)), LUT_flat), axis=1).flatten()\n",
    "    LUT_flat = np.concatenate((LUT_flat, LUT['LUTmat'].flatten()))\n",
    "    # LUT_flat = np.concatenate((LUT_flat, np.log(LUT['LUTmat']).flatten()))              #### TAKE LOG\n",
    "    BUFF_LUTs_flat[key] = LUT_flat\n",
    "    \n",
    "# create mapping from buffer ref to lab id\n",
    "BUF_ref_to_id = fo4_df.loc[fo4_df['func_id'] == 35, ['ref']]\n",
    "BUF_ref_to_id.reset_index(inplace=True, drop=True)\n",
    "BUF_ref_to_id = pd.Series(BUF_ref_to_id.index, index=BUF_ref_to_id['ref'])\n",
    "    \n",
    "# cell edge input variables (cell property)\n",
    "BUFF_LUTs_map = pd.Series([val for _, val in BUFF_LUTs_flat.items()], BUFF_LUTs_flat.keys())\n",
    "BUFF_LUTs_map.index = BUFF_LUTs_map.index.get_level_values(0).map(BUF_ref_to_id)\n",
    "assert False not in (BUFF_LUTs_map.index.values == np.arange(len(BUFF_LUTs_map.index)))\n",
    "BUFF_cell_props = np.array(list(BUFF_LUTs_map))\n",
    "\n",
    "# rise/fall caps (pin property)\n",
    "BUFF_rise_fall_caps = pd.read_csv(f'{ROOT_DIR}/datasets/{dataset_dir}/BUFF_rise_fall_caps.csv', index_col=0)\n",
    "BUFF_rise_fall_caps.index = BUFF_rise_fall_caps.index.get_level_values(0).map(BUF_ref_to_id)\n",
    "assert False not in (BUFF_rise_fall_caps.index.values == np.arange(len(BUFF_rise_fall_caps.index)))\n",
    "BUFF_pin_rf_caps = BUFF_rise_fall_caps.values\n",
    "\n",
    "with open(f'{ROOT_DIR}/datasets/{dataset_dir}/BUFF_props.pk', 'wb') as pkf:\n",
    "    pk.dump({'BUFF_cell_props': BUFF_cell_props, 'BUFF_pin_rf_caps': BUFF_pin_rf_caps}, pkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 256), (16, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFF_cell_props.shape, BUFF_pin_rf_caps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    ('node', 'cell_out', 'node'): (torch.from_numpy(pin_cell_pin_df['src_id'].values), torch.from_numpy(pin_cell_pin_df['tar_id'].values)),\n",
    "    ('node', 'net_out', 'node'): (torch.from_numpy(pin_net_pin_df['src_id'].values), torch.from_numpy(pin_net_pin_df['tar_id'].values)),\n",
    "    # ('node', 'net_in', 'node'): (torch.from_numpy(pin_net_pin_df['tar_id'].values), torch.from_numpy(pin_net_pin_df['src_id'].values))\n",
    "}\n",
    "g = dgl.heterograph(data_dict, idtype=torch.int32)\n",
    "\n",
    "g.ndata['nf'] = torch.from_numpy(node_features)\n",
    "for task_target in task_targets:\n",
    "    g.ndata[f'n_{task_target}'] = torch.from_numpy(task_df[task_target].values)\n",
    "# train mask\n",
    "g.ndata['train_mask'] = torch.from_numpy(train_mask)\n",
    "\n",
    "g.edata['ef'] = {\n",
    "    ('node', 'cell_out', 'node'): torch.from_numpy(cell_edge_info),\n",
    "    ('node', 'net_out', 'node'): torch.from_numpy(net_out_edge_info),\n",
    "    # ('node', 'net_in', 'node'): torch.from_numpy(net_in_edge_info)\n",
    "}\n",
    "\n",
    "# ids\n",
    "g.ndata['cell_id'] = torch.from_numpy(cell_id_of_pins)\n",
    "g.edata['cell_id'] = {\n",
    "    ('node', 'cell_out', 'node'): torch.from_numpy(cell_id_of_cell_out)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### construct pseudo nodes for fanin at level 1, because all nodes must start from fanout!!\n",
    "# first sort the nodes by topological order\n",
    "na, nb = g.edges(etype='net_out', form='uv')\n",
    "ca, cb = g.edges(etype='cell_out', form='uv')\n",
    "g_homo = dgl.graph((torch.cat([na, ca]).cpu(), torch.cat([nb, cb]).cpu()))\n",
    "topo = dgl.topological_nodes_generator(g_homo)\n",
    "\n",
    "# find the level 1 fanin nodes\n",
    "lv1_fanin_nodes = topo[0][g.ndata['nf'][topo[0], 1] == 0]\n",
    "\n",
    "# add pseudo nodes for fanout\n",
    "g.add_nodes(len(lv1_fanin_nodes))\n",
    "for feat in g.ndata.keys():\n",
    "    g.ndata[feat][-len(lv1_fanin_nodes):] = g.ndata[feat][lv1_fanin_nodes]\n",
    "    if feat == 'nf':\n",
    "        g.ndata[feat][-len(lv1_fanin_nodes):, 1] = 1\n",
    "\n",
    "# add edge from the pseudo nodes to the fanin nodes\n",
    "ef_feats = g.edata['ef'][('node', 'net_out', 'node')].shape[1]\n",
    "pseudo_node_ids = np.arange(len(is_fanout), len(is_fanout)+len(lv1_fanin_nodes))\n",
    "g.add_edges(pseudo_node_ids, lv1_fanin_nodes.numpy(), etype=('node', 'net_out', 'node'), data={'ef': torch.zeros(len(lv1_fanin_nodes), ef_feats, dtype=torch.float64)})\n",
    "# g.add_edges(lv1_fanin_nodes.numpy(), pseudo_node_ids, etype=('node', 'net_in', 'node'), data={'ef': torch.zeros(len(lv1_fanin_nodes), ef_feats, dtype=torch.float64)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[node data] ( = dstdata)\n",
      "nf                     torch.Size([672428, 8])\n",
      "  is_prim IO           torch.Size([672428, 1])\n",
      "  fanout(1) or in(0)   torch.Size([672428, 1])\n",
      "  dis to tlrb          torch.Size([672428, 4])\n",
      "  RF cap               torch.Size([672428, 2])\n",
      "n_tran                 torch.Size([672428])\n",
      "n_slack                torch.Size([672428])\n",
      "n_risearr              torch.Size([672428])\n",
      "n_fallarr              torch.Size([672428])\n",
      "n_is_end               torch.Size([672428])\n",
      "train_mask             torch.Size([672428])\n",
      "cell_id                torch.Size([672428])\n",
      "\n",
      "[edge data]\n",
      "ef:\n",
      "  ('node', 'cell_out', 'node')   torch.Size([447905, 256])\n",
      "  ('node', 'net_out', 'node')    torch.Size([482768, 5])\n",
      "cell_id:\n",
      "  ('node', 'cell_out', 'node')   torch.Size([447905])\n"
     ]
    }
   ],
   "source": [
    "print('[node data] ( = dstdata)')\n",
    "for nkey, ndat in g.ndata.items():\n",
    "    assert type(ndat) == torch.Tensor, 'Type must be torch.Tensor'\n",
    "    print(f'{nkey:22s} {ndat.shape}')\n",
    "    if nkey == 'nf':\n",
    "        nf = ndat\n",
    "        for fkey, frange in [('is_prim IO', [0,1]), ('fanout(1) or in(0)', [1,2]), ('dis to tlrb', [2,6]), ('RF cap', [6,8])]:\n",
    "            print(f'  {fkey:20s} {ndat[:, frange[0]:frange[1]].shape}')\n",
    "print()\n",
    "\n",
    "print('[edge data]')\n",
    "for ekey, edat in g.edata.items():\n",
    "    assert type(edat) == dict, 'Type must be dict'\n",
    "    print(f'{ekey}:')\n",
    "    for edat_key, edat_dat in edat.items():\n",
    "        print(f'  {f\"{edat_key}\":30s} {edat_dat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0, # nodes = 19778, # fanout = 19778, # fanin = 0\n",
      "level 1, # nodes = 53800, # fanout = 0, # fanin = 53800\n",
      "level 2, # nodes = 4759, # fanout = 4759, # fanin = 0\n",
      "level 3, # nodes = 11880, # fanout = 0, # fanin = 11880\n",
      "level 4, # nodes = 2195, # fanout = 2195, # fanin = 0\n",
      "level 5, # nodes = 12582, # fanout = 0, # fanin = 12582\n",
      "level 6, # nodes = 2589, # fanout = 2589, # fanin = 0\n",
      "level 7, # nodes = 14275, # fanout = 0, # fanin = 14275\n",
      "level 8, # nodes = 3056, # fanout = 3056, # fanin = 0\n",
      "level 9, # nodes = 14004, # fanout = 0, # fanin = 14004\n",
      "level 10, # nodes = 3255, # fanout = 3255, # fanin = 0\n",
      "level 11, # nodes = 12771, # fanout = 0, # fanin = 12771\n",
      "level 12, # nodes = 3337, # fanout = 3337, # fanin = 0\n",
      "level 13, # nodes = 13349, # fanout = 0, # fanin = 13349\n",
      "level 14, # nodes = 2825, # fanout = 2825, # fanin = 0\n",
      "level 15, # nodes = 12497, # fanout = 0, # fanin = 12497\n",
      "level 16, # nodes = 2491, # fanout = 2491, # fanin = 0\n",
      "level 17, # nodes = 8538, # fanout = 0, # fanin = 8538\n",
      "level 18, # nodes = 2390, # fanout = 2390, # fanin = 0\n",
      "level 19, # nodes = 8443, # fanout = 0, # fanin = 8443\n",
      "level 20, # nodes = 2562, # fanout = 2562, # fanin = 0\n",
      "level 21, # nodes = 7215, # fanout = 0, # fanin = 7215\n",
      "level 22, # nodes = 2496, # fanout = 2496, # fanin = 0\n",
      "level 23, # nodes = 8014, # fanout = 0, # fanin = 8014\n",
      "level 24, # nodes = 2560, # fanout = 2560, # fanin = 0\n",
      "level 25, # nodes = 7356, # fanout = 0, # fanin = 7356\n",
      "level 26, # nodes = 3092, # fanout = 3092, # fanin = 0\n",
      "level 27, # nodes = 8926, # fanout = 0, # fanin = 8926\n",
      "level 28, # nodes = 3873, # fanout = 3873, # fanin = 0\n",
      "level 29, # nodes = 10725, # fanout = 0, # fanin = 10725\n",
      "level 30, # nodes = 4301, # fanout = 4301, # fanin = 0\n",
      "level 31, # nodes = 12194, # fanout = 0, # fanin = 12194\n",
      "level 32, # nodes = 4997, # fanout = 4997, # fanin = 0\n",
      "level 33, # nodes = 13369, # fanout = 0, # fanin = 13369\n",
      "level 34, # nodes = 5716, # fanout = 5716, # fanin = 0\n",
      "level 35, # nodes = 13339, # fanout = 0, # fanin = 13339\n",
      "level 36, # nodes = 5961, # fanout = 5961, # fanin = 0\n",
      "level 37, # nodes = 13248, # fanout = 0, # fanin = 13248\n",
      "level 38, # nodes = 5756, # fanout = 5756, # fanin = 0\n",
      "level 39, # nodes = 12200, # fanout = 0, # fanin = 12200\n",
      "level 40, # nodes = 5418, # fanout = 5418, # fanin = 0\n",
      "level 41, # nodes = 11326, # fanout = 0, # fanin = 11326\n",
      "level 42, # nodes = 5259, # fanout = 5259, # fanin = 0\n",
      "level 43, # nodes = 11076, # fanout = 0, # fanin = 11076\n",
      "level 44, # nodes = 5225, # fanout = 5225, # fanin = 0\n",
      "level 45, # nodes = 10730, # fanout = 0, # fanin = 10730\n",
      "level 46, # nodes = 5212, # fanout = 5212, # fanin = 0\n",
      "level 47, # nodes = 10924, # fanout = 0, # fanin = 10924\n",
      "level 48, # nodes = 5403, # fanout = 5403, # fanin = 0\n",
      "level 49, # nodes = 10305, # fanout = 0, # fanin = 10305\n",
      "level 50, # nodes = 5015, # fanout = 5015, # fanin = 0\n",
      "level 51, # nodes = 8602, # fanout = 0, # fanin = 8602\n",
      "level 52, # nodes = 4119, # fanout = 4119, # fanin = 0\n",
      "level 53, # nodes = 6890, # fanout = 0, # fanin = 6890\n",
      "level 54, # nodes = 3528, # fanout = 3528, # fanin = 0\n",
      "level 55, # nodes = 5812, # fanout = 0, # fanin = 5812\n",
      "level 56, # nodes = 2842, # fanout = 2842, # fanin = 0\n",
      "level 57, # nodes = 4974, # fanout = 0, # fanin = 4974\n",
      "level 58, # nodes = 2471, # fanout = 2471, # fanin = 0\n",
      "level 59, # nodes = 4731, # fanout = 0, # fanin = 4731\n",
      "level 60, # nodes = 2087, # fanout = 2087, # fanin = 0\n",
      "level 61, # nodes = 4241, # fanout = 0, # fanin = 4241\n",
      "level 62, # nodes = 2000, # fanout = 2000, # fanin = 0\n",
      "level 63, # nodes = 3870, # fanout = 0, # fanin = 3870\n",
      "level 64, # nodes = 1818, # fanout = 1818, # fanin = 0\n",
      "level 65, # nodes = 3591, # fanout = 0, # fanin = 3591\n",
      "level 66, # nodes = 1677, # fanout = 1677, # fanin = 0\n",
      "level 67, # nodes = 3550, # fanout = 0, # fanin = 3550\n",
      "level 68, # nodes = 1550, # fanout = 1550, # fanin = 0\n",
      "level 69, # nodes = 3462, # fanout = 0, # fanin = 3462\n",
      "level 70, # nodes = 1425, # fanout = 1425, # fanin = 0\n",
      "level 71, # nodes = 3270, # fanout = 0, # fanin = 3270\n",
      "level 72, # nodes = 1313, # fanout = 1313, # fanin = 0\n",
      "level 73, # nodes = 3020, # fanout = 0, # fanin = 3020\n",
      "level 74, # nodes = 1157, # fanout = 1157, # fanin = 0\n",
      "level 75, # nodes = 2608, # fanout = 0, # fanin = 2608\n",
      "level 76, # nodes = 1161, # fanout = 1161, # fanin = 0\n",
      "level 77, # nodes = 2369, # fanout = 0, # fanin = 2369\n",
      "level 78, # nodes = 1096, # fanout = 1096, # fanin = 0\n",
      "level 79, # nodes = 1952, # fanout = 0, # fanin = 1952\n",
      "level 80, # nodes = 960, # fanout = 960, # fanin = 0\n",
      "level 81, # nodes = 1714, # fanout = 0, # fanin = 1714\n",
      "level 82, # nodes = 831, # fanout = 831, # fanin = 0\n",
      "level 83, # nodes = 1551, # fanout = 0, # fanin = 1551\n",
      "level 84, # nodes = 755, # fanout = 755, # fanin = 0\n",
      "level 85, # nodes = 1340, # fanout = 0, # fanin = 1340\n",
      "level 86, # nodes = 668, # fanout = 668, # fanin = 0\n",
      "level 87, # nodes = 1015, # fanout = 0, # fanin = 1015\n",
      "level 88, # nodes = 562, # fanout = 562, # fanin = 0\n",
      "level 89, # nodes = 807, # fanout = 0, # fanin = 807\n",
      "level 90, # nodes = 477, # fanout = 477, # fanin = 0\n",
      "level 91, # nodes = 710, # fanout = 0, # fanin = 710\n",
      "level 92, # nodes = 406, # fanout = 406, # fanin = 0\n",
      "level 93, # nodes = 659, # fanout = 0, # fanin = 659\n",
      "level 94, # nodes = 380, # fanout = 380, # fanin = 0\n",
      "level 95, # nodes = 618, # fanout = 0, # fanin = 618\n",
      "level 96, # nodes = 371, # fanout = 371, # fanin = 0\n",
      "level 97, # nodes = 664, # fanout = 0, # fanin = 664\n",
      "level 98, # nodes = 352, # fanout = 352, # fanin = 0\n",
      "level 99, # nodes = 955, # fanout = 0, # fanin = 955\n",
      "level 100, # nodes = 468, # fanout = 468, # fanin = 0\n",
      "level 101, # nodes = 1217, # fanout = 0, # fanin = 1217\n",
      "level 102, # nodes = 540, # fanout = 540, # fanin = 0\n",
      "level 103, # nodes = 1111, # fanout = 0, # fanin = 1111\n",
      "level 104, # nodes = 386, # fanout = 386, # fanin = 0\n",
      "level 105, # nodes = 640, # fanout = 0, # fanin = 640\n",
      "level 106, # nodes = 267, # fanout = 267, # fanin = 0\n",
      "level 107, # nodes = 405, # fanout = 0, # fanin = 405\n",
      "level 108, # nodes = 131, # fanout = 131, # fanin = 0\n",
      "level 109, # nodes = 286, # fanout = 0, # fanin = 286\n",
      "level 110, # nodes = 125, # fanout = 125, # fanin = 0\n",
      "level 111, # nodes = 263, # fanout = 0, # fanin = 263\n",
      "level 112, # nodes = 135, # fanout = 135, # fanin = 0\n",
      "level 113, # nodes = 313, # fanout = 0, # fanin = 313\n",
      "level 114, # nodes = 161, # fanout = 161, # fanin = 0\n",
      "level 115, # nodes = 567, # fanout = 0, # fanin = 567\n",
      "level 116, # nodes = 247, # fanout = 247, # fanin = 0\n",
      "level 117, # nodes = 681, # fanout = 0, # fanin = 681\n",
      "level 118, # nodes = 280, # fanout = 280, # fanin = 0\n",
      "level 119, # nodes = 724, # fanout = 0, # fanin = 724\n",
      "level 120, # nodes = 382, # fanout = 382, # fanin = 0\n",
      "level 121, # nodes = 902, # fanout = 0, # fanin = 902\n",
      "level 122, # nodes = 442, # fanout = 442, # fanin = 0\n",
      "level 123, # nodes = 1059, # fanout = 0, # fanin = 1059\n",
      "level 124, # nodes = 493, # fanout = 493, # fanin = 0\n",
      "level 125, # nodes = 1407, # fanout = 0, # fanin = 1407\n",
      "level 126, # nodes = 546, # fanout = 546, # fanin = 0\n",
      "level 127, # nodes = 1729, # fanout = 0, # fanin = 1729\n",
      "level 128, # nodes = 539, # fanout = 539, # fanin = 0\n",
      "level 129, # nodes = 1453, # fanout = 0, # fanin = 1453\n",
      "level 130, # nodes = 517, # fanout = 517, # fanin = 0\n",
      "level 131, # nodes = 1191, # fanout = 0, # fanin = 1191\n",
      "level 132, # nodes = 503, # fanout = 503, # fanin = 0\n",
      "level 133, # nodes = 1206, # fanout = 0, # fanin = 1206\n",
      "level 134, # nodes = 407, # fanout = 407, # fanin = 0\n",
      "level 135, # nodes = 1059, # fanout = 0, # fanin = 1059\n",
      "level 136, # nodes = 442, # fanout = 442, # fanin = 0\n",
      "level 137, # nodes = 1197, # fanout = 0, # fanin = 1197\n",
      "level 138, # nodes = 614, # fanout = 614, # fanin = 0\n",
      "level 139, # nodes = 1483, # fanout = 0, # fanin = 1483\n",
      "level 140, # nodes = 574, # fanout = 574, # fanin = 0\n",
      "level 141, # nodes = 1813, # fanout = 0, # fanin = 1813\n",
      "level 142, # nodes = 868, # fanout = 868, # fanin = 0\n",
      "level 143, # nodes = 2557, # fanout = 0, # fanin = 2557\n",
      "level 144, # nodes = 1138, # fanout = 1138, # fanin = 0\n",
      "level 145, # nodes = 2928, # fanout = 0, # fanin = 2928\n",
      "level 146, # nodes = 1391, # fanout = 1391, # fanin = 0\n",
      "level 147, # nodes = 3343, # fanout = 0, # fanin = 3343\n",
      "level 148, # nodes = 1536, # fanout = 1536, # fanin = 0\n",
      "level 149, # nodes = 3024, # fanout = 0, # fanin = 3024\n",
      "level 150, # nodes = 1447, # fanout = 1447, # fanin = 0\n",
      "level 151, # nodes = 2575, # fanout = 0, # fanin = 2575\n",
      "level 152, # nodes = 1356, # fanout = 1356, # fanin = 0\n",
      "level 153, # nodes = 2214, # fanout = 0, # fanin = 2214\n",
      "level 154, # nodes = 1260, # fanout = 1260, # fanin = 0\n",
      "level 155, # nodes = 2206, # fanout = 0, # fanin = 2206\n",
      "level 156, # nodes = 1256, # fanout = 1256, # fanin = 0\n",
      "level 157, # nodes = 2374, # fanout = 0, # fanin = 2374\n",
      "level 158, # nodes = 1258, # fanout = 1258, # fanin = 0\n",
      "level 159, # nodes = 2725, # fanout = 0, # fanin = 2725\n",
      "level 160, # nodes = 1334, # fanout = 1334, # fanin = 0\n",
      "level 161, # nodes = 3026, # fanout = 0, # fanin = 3026\n",
      "level 162, # nodes = 1605, # fanout = 1605, # fanin = 0\n",
      "level 163, # nodes = 3628, # fanout = 0, # fanin = 3628\n",
      "level 164, # nodes = 1339, # fanout = 1339, # fanin = 0\n",
      "level 165, # nodes = 3440, # fanout = 0, # fanin = 3440\n",
      "level 166, # nodes = 1457, # fanout = 1457, # fanin = 0\n",
      "level 167, # nodes = 4931, # fanout = 0, # fanin = 4931\n",
      "level 168, # nodes = 1624, # fanout = 1624, # fanin = 0\n",
      "level 169, # nodes = 6424, # fanout = 0, # fanin = 6424\n",
      "level 170, # nodes = 1936, # fanout = 1936, # fanin = 0\n",
      "level 171, # nodes = 6642, # fanout = 0, # fanin = 6642\n",
      "level 172, # nodes = 1960, # fanout = 1960, # fanin = 0\n",
      "level 173, # nodes = 5694, # fanout = 0, # fanin = 5694\n",
      "level 174, # nodes = 2149, # fanout = 2149, # fanin = 0\n",
      "level 175, # nodes = 4151, # fanout = 0, # fanin = 4151\n",
      "level 176, # nodes = 1606, # fanout = 1606, # fanin = 0\n",
      "level 177, # nodes = 2531, # fanout = 0, # fanin = 2531\n",
      "level 178, # nodes = 955, # fanout = 955, # fanin = 0\n",
      "level 179, # nodes = 1120, # fanout = 0, # fanin = 1120\n",
      "level 180, # nodes = 396, # fanout = 396, # fanin = 0\n",
      "level 181, # nodes = 405, # fanout = 0, # fanin = 405\n",
      "level 182, # nodes = 63, # fanout = 63, # fanin = 0\n",
      "level 183, # nodes = 63, # fanout = 0, # fanin = 63\n"
     ]
    }
   ],
   "source": [
    "na, nb = g.edges(etype='net_out', form='uv')\n",
    "ca, cb = g.edges(etype='cell_out', form='uv')\n",
    "g_homo = dgl.graph((torch.cat([na, ca]).cpu(), torch.cat([nb, cb]).cpu()))\n",
    "topo = dgl.topological_nodes_generator(g_homo)\n",
    "g_homo.ndata['fanout'] = g.ndata['nf'][:, 1].cpu()\n",
    "\n",
    "# assert that even/odd levels -> only fanout/fanin nodes\n",
    "for li, nodes in enumerate(topo):\n",
    "    fanout = g_homo.ndata['fanout'][nodes].numpy()\n",
    "    assert (li % 2 == 0 and (fanout == 0).sum() == 0) or (li % 2 == 1 and (fanout == 1).sum() == 0)\n",
    "\n",
    "# add pseudo node to last level if odd number of levels\n",
    "if len(topo) % 2 == 1:\n",
    "    # get the fanout nodes at the last level\n",
    "    fin_lv_fanout_nodes = topo[-1]\n",
    "\n",
    "    # add pseudo nodes for fanin\n",
    "    g.add_nodes(len(fin_lv_fanout_nodes))\n",
    "    for feat in g.ndata.keys():\n",
    "        g.ndata[feat][-len(fin_lv_fanout_nodes):] = g.ndata[feat][fin_lv_fanout_nodes]\n",
    "        if feat == 'nf':\n",
    "            g.ndata[feat][-len(fin_lv_fanout_nodes):, 1] = 0\n",
    "\n",
    "    # add edge from the fanout nodes to the pseudo nodes\n",
    "    ef_feats = g.edata['ef'][('node', 'net_out', 'node')].shape[1]\n",
    "    pseudo_node_ids = np.arange(len(g.ndata['nf'])-len(fin_lv_fanout_nodes), len(g.ndata['nf']))\n",
    "    g.add_edges(fin_lv_fanout_nodes.numpy(), pseudo_node_ids, etype=('node', 'net_out', 'node'), data={'ef': torch.zeros(len(fin_lv_fanout_nodes), ef_feats, dtype=torch.float64)})\n",
    "    # g.add_edges(pseudo_node_ids, fin_lv_fanout_nodes.numpy(), etype=('node', 'net_in', 'node'), data={'ef': torch.zeros(len(fin_lv_fanout_nodes), ef_feats, dtype=torch.float64)})\n",
    "\n",
    "na, nb = g.edges(etype='net_out', form='uv')\n",
    "ca, cb = g.edges(etype='cell_out', form='uv')\n",
    "g_homo = dgl.graph((torch.cat([na, ca]).cpu(), torch.cat([nb, cb]).cpu()))\n",
    "topo = dgl.topological_nodes_generator(g_homo)\n",
    "g_homo.ndata['fanout'] = g.ndata['nf'][:, 1].cpu()\n",
    "\n",
    "assert len(topo) % 2 == 0\n",
    "\n",
    "### inspect the topography!\n",
    "for li, nodes in enumerate(topo):\n",
    "    fanout = g_homo.ndata['fanout'][nodes].numpy()\n",
    "    print(f'level {li}, # nodes = {len(nodes)}, # fanout = {(fanout == 1).sum()}, # fanin = {(fanout == 0).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Graph(num_nodes={'node': 657377},\n",
       "       num_edges={('node', 'cell_out', 'node'): 447458, ('node', 'net_out', 'node'): 468169},\n",
       "       metagraph=[('node', 'node', 'cell_out'), ('node', 'node', 'net_out')]),\n",
       " Graph(num_nodes={'node': 9077},\n",
       "       num_edges={('node', 'cell_out', 'node'): 279, ('node', 'net_out', 'node'): 8797},\n",
       "       metagraph=[('node', 'node', 'cell_out'), ('node', 'node', 'net_out')]),\n",
       " Graph(num_nodes={'node': 4163},\n",
       "       num_edges={('node', 'cell_out', 'node'): 33, ('node', 'net_out', 'node'): 4129},\n",
       "       metagraph=[('node', 'node', 'cell_out'), ('node', 'node', 'net_out')]),\n",
       " Graph(num_nodes={'node': 903},\n",
       "       num_edges={('node', 'cell_out', 'node'): 67, ('node', 'net_out', 'node'): 835},\n",
       "       metagraph=[('node', 'node', 'cell_out'), ('node', 'node', 'net_out')]),\n",
       " Graph(num_nodes={'node': 903},\n",
       "       num_edges={('node', 'cell_out', 'node'): 67, ('node', 'net_out', 'node'): 835},\n",
       "       metagraph=[('node', 'node', 'cell_out'), ('node', 'node', 'net_out')])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract subgraphs\n",
    "MIN_SUB_GRAPH_NODES = 100\n",
    "na, nb = g.edges(etype='net_out', form='uv')\n",
    "ca, cb = g.edges(etype='cell_out', form='uv')\n",
    "g_homo = dgl.graph((torch.cat([na, ca]).cpu(), torch.cat([nb, cb]).cpu()))\n",
    "nx_g = g_homo.to_networkx().to_undirected()\n",
    "comps = nx.connected_components(nx_g)\n",
    "comps = [np.array(sorted(list(comp))) for comp in comps]\n",
    "sub_gs = [g.subgraph(nodes) for nodes in comps if len(nodes) >= MIN_SUB_GRAPH_NODES]\n",
    "sub_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add net_in edge with values opposite from net_out edges for each subgraph\n",
    "bidir_gs = []\n",
    "for sub_g in sub_gs:\n",
    "    net_out_src, net_out_dst = sub_g.edges(etype='net_out')\n",
    "    data_dict = {\n",
    "        ('node', 'cell_out', 'node'): sub_g.edges(etype='cell_out'),\n",
    "        ('node', 'net_out', 'node'): sub_g.edges(etype='net_out'),\n",
    "        ('node', 'net_in', 'node'): (net_out_dst, net_out_src)\n",
    "    }\n",
    "    bidir_g = dgl.heterograph(data_dict, idtype=torch.int32)\n",
    "\n",
    "    for key in sub_g.ndata.keys():\n",
    "        bidir_g.ndata[key] = sub_g.ndata[key]\n",
    "\n",
    "    bidir_g.edata['ef'] = {\n",
    "        ('node', 'cell_out', 'node'): sub_g.edata['ef'][('node', 'cell_out', 'node')],\n",
    "        ('node', 'net_out', 'node'): sub_g.edata['ef'][('node', 'net_out', 'node')],\n",
    "        ('node', 'net_in', 'node'): -sub_g.edata['ef'][('node', 'net_out', 'node')]\n",
    "    }\n",
    "\n",
    "    bidir_g.edata['cell_id'] = {\n",
    "        ('node', 'cell_out', 'node'): sub_g.edata['cell_id'][('node', 'cell_out', 'node')]\n",
    "    }\n",
    "\n",
    "    bidir_gs.append(bidir_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dgl graph to original dataset location\n",
    "dgl.save_graphs(f'{design_dir}/graph.dgl', bidir_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
